{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technology Conference Aggregation and Analysis Project\n",
    "### Data Mining and Data Ingest\n",
    "### April 28, 2020\n",
    "\n",
    " ![datamining](datamining.png)\n",
    " \n",
    "## Importing Modules and Retrieving Custom Functions\n",
    "\n",
    "For this project, I chose to primarily use the Python requests modules to interact with several conference websites and BeautifulSoup to parse and format/extract data. In some cases where rendering of client-side JavaScript was required to interact with pages, I used Selenium webdriver to access data through an automated headless Firefox browser window. I used Azure Cosmos DB as a cloud-based database-as-a-service (DBaaS) with the Python Azure software development kit (SDK) to interact with the database where I stored intermediate results from the website mining.\n",
    "\n",
    "To streamline web scraping, I created multiple custom functions for generating scraper script metadata, starting preconfigured headless browsers and setting time limits for scripts. This allows a script to be scheduled on a laptop, virtual machine, or even an inexpensive single-board computer (SBC) such as a Raspberry Pi. A Pi is a good choice for longer term projects because scraping from sites is not computation intensive, but requires time to fetch thousands of individual pages without overloading the web servers. There are ARM-compatible web drivers that can automate Firefox for this purpose.\n",
    "\n",
    "## Event Websites\n",
    "There are a number of sites that aggregate technology conferences, although no one site has an exhaustive worldwide list of events. This site has thousands of events broken out by city/country and broad category, such as technology conference. The location/categories are listed at links at https://www.eventbrite.com/directory/sitemap/. This notebook is focused on this site, but the same principles apply to other sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from bs4 import BeautifulSoup\n",
    "from random import shuffle\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import socket\n",
    "import io\n",
    "from IPython.core.display import Image, display, HTML\n",
    "from PIL import Image as Im\n",
    "\n",
    "# Set parameters\n",
    "TIME_LIMIT = 3600\n",
    "WAIT_TIME = 4\n",
    "\n",
    "# Demo flag. Set to False to turn on/off additional feedback\n",
    "DEMO = True\n",
    "\n",
    "# Get local folder and add project folder to PATH\n",
    "workingdir = os.getcwd()\n",
    "sys.path.insert(0, workingdir)\n",
    "parentdir = os.path.dirname(workingdir)\n",
    "sys.path.insert(0, parentdir)\n",
    "\n",
    "# Import custom modules\n",
    "from utils.scraping import headless_browser, date_parse, update_time, scraper_info\n",
    "\n",
    "# Hides code in HTML output\n",
    "HTML('''<script>\n",
    "    code_show=true; \n",
    "    function code_toggle() {\n",
    "     if (code_show){\n",
    "     $('div.input').hide();\n",
    "     } else {\n",
    "     $('div.input').show();\n",
    "     }\n",
    "     code_show = !code_show\n",
    "    } \n",
    "    $( document ).ready(code_toggle);\n",
    "    </script>\n",
    "    <form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch path of notebook. Not required for regular Python scripts\n",
    "__file__ = os.path.join(os.getcwd(), 'datamining.ipynb')\n",
    "print(__file__)\n",
    "\n",
    "# Get scraper info\n",
    "scraperip, hostname, scriptname, dtg, start_time = scraper_info(__file__, DEMO)\n",
    "\n",
    "# Start headless browser and fetch page info\n",
    "browser = headless_browser(__file__, DEMO)\n",
    "useragent = browser.execute_script(\"return navigator.userAgent;\")\n",
    "print(\"XXXXXXXXXXXXXXX\" + useragent[-30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Websites with Python and Selenium\n",
    "We could use the requests library to get individual events based on URL, but to get a list of URLs, we need to use the site's search tool and filters. To avoid doing this manually and programatically navigate the search page, we will use Selenium.\n",
    "\n",
    "Selenium is a tool that automates browsers, such as Chrome or Firefox. Selenium is primarily used for testing websites, but can also be used to navigate and scrape data from them. The Selenium driver for Firefox also supports headless browsing, meaning we can launch the browser in the background without graphical resources and execute and interact with any Javascript-based rendering or form elements. Web scraping through an automated browser is slower than executing direct requests through Python, but often the limiting factor for scraping is Internet bandwidth and web server resources. In addition, with browser automation we can do more than retrieve just the source HTML from the page including rendering and creating screenshots of webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch webpage\n",
    "srcurl = 'https://www.eventbrite.com/d/ma--medford/science-and-tech--conferences/'\n",
    "browser.get(srcurl)\n",
    "time.sleep(2)\n",
    "\n",
    "# Render screenshot of page\n",
    "png1 = browser.get_screenshot_as_png()\n",
    "display(Im.open(io.BytesIO(png1)).crop((0,0,1200,800)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the search page results are loaded with Selenium, we need to determine the pagination count and then fetch the event URLs from each list of paginated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find pagination number of result pages to fetch\n",
    "src = browser.page_source\n",
    "soup = BeautifulSoup(src, 'html.parser')\n",
    "try:\n",
    "    pages = int(soup.find(\n",
    "        \"div\", attrs={\"data-spec\": \"paginator__last-page-link\"}).decode_contents().replace('.', ''))\n",
    "except Exception:\n",
    "    try:\n",
    "        pages = int(soup.find(\"li\", attrs={\n",
    "                    \"data-spec\": \"eds-pagination__navigation-minimal\"}).decode_contents().split('of')[-1].strip())\n",
    "    except Exception:\n",
    "        pages = 1\n",
    "print(str(pages) + ' pages found.')\n",
    "\n",
    "# Process pages\n",
    "foundurls = []\n",
    "for page1 in range(1, pages + 1):\n",
    "    dtg = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    jsons = []\n",
    "    try:\n",
    "        in1 = list(\n",
    "            set(soup.find_all(\"script\", attrs={\"type\": \"application/ld+json\"})))\n",
    "        jsons = json.loads(str(in1[0].decode_contents()))\n",
    "        print(str(len(jsons)) + ' events found on current page. ' + str(len(foundurls) + len(jsons)) + ' total events founds.')\n",
    "    except Exception:\n",
    "        try:\n",
    "            browser.get(srcurl)\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(10)\n",
    "        src = browser.page_source\n",
    "        soup = BeautifulSoup(src, 'html.parser')\n",
    "        jsons = []\n",
    "        try:\n",
    "            in1 = list(\n",
    "                set(soup.find_all(\"script\", attrs={\"type\": \"application/ld+json\"})))\n",
    "            jsons = json.loads(str(in1[0].decode_contents()))\n",
    "            print(str(len(jsons)) + ' events found on current page ' + str(len(foundurls) + len(jsons)) + ' total events founds.')\n",
    "        except Exception:\n",
    "            jsons = []\n",
    "            print('no events found...')\n",
    "    for json1 in jsons:\n",
    "        dtg = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        result1 = {}\n",
    "        result1['dtg'] = dtg\n",
    "        result1['hostname'] = hostname\n",
    "        result1['scriptname'] = scriptname\n",
    "        result1['srcurl'] = srcurl\n",
    "        result1['useragent'] = useragent\n",
    "        result1['pagetitle'] = soup.title.decode_contents().strip()\n",
    "        try:\n",
    "            result1['name'] = json1['name'].strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result1['eventurl'] = json1['url']\n",
    "            result1['eid'] = result1['eventurl'].split('-')[-1]\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result1['description'] = json1['description'].strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result1['startDate'] = str(json1['startDate']) + ' 00:00:00'\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result1['endDate'] = json1['endDate'] + ' 00:00:00'\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result1['lowPrice'] = str(\n",
    "                json1['offers']['priceCurrency']) + ' ' + str(json1['offers']['lowPrice'])\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result1['highPrice'] = str(\n",
    "                json1['offers']['priceCurrency']) + ' ' + str(json1['offers']['highPrice'])\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result1['venuename'] = json1['location']['name'].strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result1['streetAddress'] = json1['location']['address']['streetAddress'].strip(\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result1['locality'] = json1['location']['address']['addressLocality'].strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result1['region'] = json1['location']['address']['addressRegion'].strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result1['country'] = json1['location']['address']['addressCountry']['name'].strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result1['latitude'] = round(\n",
    "                float(json1['location']['geo']['latitude']), 7)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result1['longitude'] = round(\n",
    "                float(json1['location']['geo']['longitude']), 7)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if result1['name'] != '' and not result1['eventurl'] in foundurls:\n",
    "            foundurls += [result1['eventurl']]\n",
    "    elapsed_time = int(time.time() - start_time)\n",
    "    if page1 < pages:\n",
    "        print(str(elapsed_time) + ' seconds elapsed. starting page number ' + str(page1 + 1) + ' of ' + str(pages))\n",
    "        try:\n",
    "            browser.get(srcurl + '?page=' + str(page1))\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(3)\n",
    "        src = browser.page_source\n",
    "        soup = BeautifulSoup(src, 'html.parser')\n",
    "        # Increment and show elapsed time until limit reached\n",
    "        update_time(start_time, TIME_LIMIT, WAIT_TIME)\n",
    "\n",
    "# Print sample of URLs\n",
    "print('\\n'.join(foundurls[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Individual Event Pages\n",
    "With a list of event URLs, we can go to the next step and fetch each page. For the purpose of this demonstration, we will only look at one page, as scraping all the events from the website would take hours or even days. While we're using Selenium for this page to render a screenshot, we use BeautifulSoup to parse the HTML. BeautifulSoup is a library that can turn the source HTML into a searchable object tree in Python. In addition, some sites that use JQuery or other JavaScript libraries to search and render results may include formatted JSONs hidden in the source code itself. It takes additional steps including the Python JSON library, but this allows us to find structured metadata about events which can be converted to dictionaries. From there, we can selectively use that data to build and output a result dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch webpage\n",
    "srcurl = 'https://www.eventbrite.com/e/global-artificial-intelligence-conference-boston-october-2020-tickets-81221478629'\n",
    "browser.get(srcurl)\n",
    "time.sleep(2)\n",
    "\n",
    "# Render screenshot of page\n",
    "png1 = browser.get_screenshot_as_png()\n",
    "display(Im.open(io.BytesIO(png1)).crop((0,0,1200,800)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src = browser.page_source\n",
    "soup = BeautifulSoup(src, 'html.parser')\n",
    "result = {}\n",
    "result['srcurl'] = srcurl\n",
    "result['eventurl'] = srcurl\n",
    "result['dtg'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# Findings field values using BeautifulSoup\n",
    "if not DEMO:\n",
    "    try:\n",
    "        result['scraperip'] = scraperip\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        result['hostname'] = hostname\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    result['scraperip'] = 'XXX.XXX.XXX.XXX'\n",
    "    result['hostname'] = 'XXXX'\n",
    "try:\n",
    "    result['scriptname'] = scriptname\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['eid'] = result['eventurl'].split('-')[-1]\n",
    "except Exception:\n",
    "    pass\n",
    "result['name'] = ''\n",
    "try:\n",
    "    result['name'] = json.loads(soup.find(\"script\", attrs={\n",
    "                                \"type\": \"application/ld+json\"}).decode_contents(), strict=False)['name'].strip()\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['description'] = json.loads(soup.find(\"script\", attrs={\n",
    "                                       \"type\": \"application/ld+json\"}).decode_contents(), strict=False)['description'][:2000].strip()\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['startDate'] = json.loads(soup.find(\"script\", attrs={\n",
    "                                     \"type\": \"application/ld+json\"}).decode_contents(), strict=False)['startDate'].split('T')[0] + ' 00:00:00'\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['endDate'] = json.loads(soup.find(\"script\", attrs={\n",
    "                                   \"type\": \"application/ld+json\"}).decode_contents(), strict=False)['endDate'].split('T')[0] + ' 00:00:00'\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['venuename'] = json.loads(soup.find(\"script\", attrs={\n",
    "                                     \"type\": \"application/ld+json\"}).decode_contents(), strict=False)['location']['name']\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['streetAddress'] = json.loads(soup.find(\"script\", attrs={\n",
    "                                         \"type\": \"application/ld+json\"}).decode_contents(), strict=False)['location']['address']['streetAddress']\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['locality'] = json.loads(soup.find(\"script\", attrs={\n",
    "                                    \"type\": \"application/ld+json\"}).decode_contents(), strict=False)['location']['address']['addressLocality']\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['region'] = json.loads(soup.find(\"script\", attrs={\n",
    "                                  \"type\": \"application/ld+json\"}).decode_contents(), strict=False)['location']['address']['addressRegion']\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['country'] = json.loads(soup.find(\"script\", attrs={\n",
    "                                   \"type\": \"application/ld+json\"}).decode_contents(), strict=False)['location']['address']['addressCountry']\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['latitude'] = soup.find(\n",
    "        \"meta\", attrs={\"property\": \"event:location:latitude\"})['content'].strip()\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['longitude'] = soup.find(\n",
    "        \"meta\", attrs={\"property\": \"event:location:longitude\"})['content'].strip()\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['zipcode'] = json.loads(soup.find(\"script\", attrs={\n",
    "                                   \"type\": \"application/ld+json\"}).decode_contents(), strict=False)['location']['address']['postalCode'].strip()\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['category'] = soup.find(\"a\", attrs={\n",
    "                                   \"data-event-category\": \"listing\"}).parent.parent.get_text('|').replace('\\n|', '').strip()[:-1]\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['lowPrice'] = str(json.loads(soup.find(\"script\", attrs={\"type\": \"application/ld+json\"}).decode_contents(), strict=False)['offers'][0]['priceCurrency']) + ' ' + str(\n",
    "        json.loads(soup.find(\"script\", attrs={\"type\": \"application/ld+json\"}).decode_contents(), strict=False)['offers'][0]['lowPrice'])\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['highPrice'] = str(json.loads(soup.find(\"script\", attrs={\"type\": \"application/ld+json\"}).decode_contents(), strict=False)['offers'][0]['priceCurrency']) + ' ' + str(\n",
    "        json.loads(soup.find(\"script\", attrs={\"type\": \"application/ld+json\"}).decode_contents(), strict=False)['offers'][0]['highPrice'])\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    result['organizer'] = json.loads(soup.find(\"script\", attrs={\n",
    "                                     \"type\": \"application/ld+json\"}).decode_contents(), strict=False)['organizer']['name'].strip()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(json.dumps(result, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once the results are formatted as a dictionary they can be saved as a JSON and/or uploaded to a document database such as Cosmos DB or Mongo DB. Document DBs are a type of NoSQL database which can be schema-less. The advantage with this approach is that we can build formatted document entries with only the metadata found from each site. As new sites are added with different metadata, document databases are flexible enough to incorporate new data since there is no fixed schema. However, it is helpful to adhere to data validation and reuse common key names such as latitude/longitude, so that the data can be easily retrieved and analyzed later on. I include some additional script metadata to help with troubleshooting parsing issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1597690597855"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}